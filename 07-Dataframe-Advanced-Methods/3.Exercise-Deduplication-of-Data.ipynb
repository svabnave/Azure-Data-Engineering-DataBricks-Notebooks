{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d34bfe28-243d-49d1-a5ff-7095ff45bd74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Lab Exercise\n",
    "## De-Duping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e3e3f3-6d52-460e-8b44-6663b5e97c09",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Instructions\n",
    "\n",
    "In this exercise, we're doing ETL on a file we've received from some customer. That file contains data about people, including:\n",
    "\n",
    "* first, middle and last names\n",
    "* gender\n",
    "* birth date\n",
    "* Social Security number\n",
    "* salary\n",
    "\n",
    "But, as is unfortunately common in data we get from this customer, the file contains some duplicate records. Worse:\n",
    "\n",
    "* In some of the records, the names are mixed case (e.g., \"Carol\"), while in others, they are uppercase (e.g., \"CAROL\"). \n",
    "* The Social Security numbers aren't consistent, either. Some of them are hyphenated (e.g., \"992-83-4829\"), while others are missing hyphens (\"992834829\").\n",
    "\n",
    "The name fields are guaranteed to match, if you disregard character case, and the birth dates will also match. (The salaries will match, as well,\n",
    "and the Social Security Numbers *would* match, if they were somehow put in the same format).\n",
    "\n",
    "Your job is to remove the duplicate records. The specific requirements of your job are:\n",
    "\n",
    "* Remove duplicates. It doesn't matter which record you keep; it only matters that you keep one of them.\n",
    "* Preserve the data format of the columns. For example, if you write the first name column in all lower-case, you haven't met this requirement.\n",
    "* Write the result as a Parquet file, as designated by *destFile*.\n",
    "* The final Parquet \"file\" must contain 8 part files (8 files ending in \".parquet\").\n",
    "\n",
    "<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** The initial dataset contains 103,000 records.<br/>\n",
    "The de-duplicated result haves 100,000 records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b7eaee-ebe9-4f1c-b885-57f8399258da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n",
    "\n",
    "Run the following cell to configure our \"classroom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e782ebf-7ddf-47f6-9f1c-3bc8d828820b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Initialized classroom variables & functions..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Initialized classroom variables & functions...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d2ebd2-a3d4-483c-8537-ecc3c9eb2861",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/training/ has been unmounted.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Mounted datasets to <b>/mnt/training</b> from <b>wasbs://training@dbtrainwesteurope.blob.core.windows.net/<b>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Mounted datasets to <b>/mnt/training</b> from <b>wasbs://training@dbtrainwesteurope.blob.core.windows.net/<b>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mount \"/mnt/training\" again using \"%run \"./Includes/Dataset-Mounts-New\"\" if it is failed in \"./Includes/Classroom-Setup\"\n",
    "try:\n",
    "    files = dbutils.fs.ls(\"/mnt/training\")\n",
    "except:\n",
    "    dbutils.fs.unmount('/mnt/training/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4035c874-c32a-41e8-b3e6-75721468a5bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Created user-specific database"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Created user-specific database",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Using the database <b style=\"color:green\">vishal_abnave_borregaard_com_db</b>."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Using the database <b style=\"color:green\">vishal_abnave_borregaard_com_db</b>.",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "All done!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Dataset-Mounts-New\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9423f4fe-e469-41d6-a2f3-d442b31f3a0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets are mounted\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "  <div>Initializing lab environment:</div>\n",
       "  <li>Declared <b style=\"color:green\">clearYourResults(<i>passedOnly:Boolean=true</i>)</b></li>\n",
       "  <li>Declared <b style=\"color:green\">validateYourSchema(<i>what:String, df:DataFrame, expColumnName:String, expColumnType:String</i>)</b></li>\n",
       "  <li>Declared <b style=\"color:green\">validateYourAnswer(<i>what:String, expectedHash:Int, answer:Any</i>)</b></li>\n",
       "  <li>Declared <b style=\"color:green\">summarizeYourResults()</b></li>\n",
       "  <li>Declared <b style=\"color:green\">logYourTest(<i>path:String, name:String, value:Double</i>)</b></li>\n",
       "  <li>Declared <b style=\"color:green\">loadYourTestResults(<i>path:String</i>)</b> returns <b style=\"color:green\">DataFrame</b></li>\n",
       "  <li>Declared <b style=\"color:green\">loadYourTestMap(<i>path:String</i>)</b> returns <b style=\"color:green\">Map[String,Double]</b></li>\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n  <div>Initializing lab environment:</div>\n  <li>Declared <b style=\"color:green\">clearYourResults(<i>passedOnly:Boolean=true</i>)</b></li>\n  <li>Declared <b style=\"color:green\">validateYourSchema(<i>what:String, df:DataFrame, expColumnName:String, expColumnType:String</i>)</b></li>\n  <li>Declared <b style=\"color:green\">validateYourAnswer(<i>what:String, expectedHash:Int, answer:Any</i>)</b></li>\n  <li>Declared <b style=\"color:green\">summarizeYourResults()</b></li>\n  <li>Declared <b style=\"color:green\">logYourTest(<i>path:String, name:String, value:Double</i>)</b></li>\n  <li>Declared <b style=\"color:green\">loadYourTestResults(<i>path:String</i>)</b> returns <b style=\"color:green\">DataFrame</b></li>\n  <li>Declared <b style=\"color:green\">loadYourTestMap(<i>path:String</i>)</b> returns <b style=\"color:green\">Map[String,Double]</b></li>\n",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Initialize-Labs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff70f218-f289-4395-abbf-2c905de18f21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Hints\n",
    "\n",
    "* Use the <a href=\"http://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\">API docs</a>. Specifically, you might find \n",
    "  <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\" target=\"_blank\">DataFrame</a> and\n",
    "  <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\" target=\"_blank\">functions</a> to be helpful.\n",
    "* It's helpful to look at the file first, so you can check the format. `dbutils.fs.head()` (or just `%fs head`) is a big help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1094f86-7eec-43cf-81a2-5640d4881796",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records: 100,000\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/user/vishal.abnave@borregaard.com/people.parquet/_SUCCESS</td><td>_SUCCESS</td><td>0</td><td>1684687267000</td></tr><tr><td>dbfs:/user/vishal.abnave@borregaard.com/people.parquet/_committed_3162966097481226355</td><td>_committed_3162966097481226355</td><td>624</td><td>1684687267000</td></tr><tr><td>dbfs:/user/vishal.abnave@borregaard.com/people.parquet/_started_3162966097481226355</td><td>_started_3162966097481226355</td><td>0</td><td>1684687265000</td></tr><tr><td>dbfs:/user/vishal.abnave@borregaard.com/people.parquet/part-00000-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-223-1-c000.snappy.parquet</td><td>part-00000-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-223-1-c000.snappy.parquet</td><td>408085</td><td>1684687266000</td></tr><tr><td>dbfs:/user/vishal.abnave@borregaard.com/people.parquet/part-00001-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-224-1-c000.snappy.parquet</td><td>part-00001-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-224-1-c000.snappy.parquet</td><td>752260</td><td>1684687266000</td></tr><tr><td>dbfs:/user/vishal.abnave@borregaard.com/people.parquet/part-00002-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-225-1-c000.snappy.parquet</td><td>part-00002-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-225-1-c000.snappy.parquet</td><td>408748</td><td>1684687266000</td></tr><tr><td>dbfs:/user/vishal.abnave@borregaard.com/people.parquet/part-00003-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-226-1-c000.snappy.parquet</td><td>part-00003-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-226-1-c000.snappy.parquet</td><td>748279</td><td>1684687266000</td></tr><tr><td>dbfs:/user/vishal.abnave@borregaard.com/people.parquet/part-00004-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-227-1-c000.snappy.parquet</td><td>part-00004-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-227-1-c000.snappy.parquet</td><td>405791</td><td>1684687266000</td></tr><tr><td>dbfs:/user/vishal.abnave@borregaard.com/people.parquet/part-00005-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-228-1-c000.snappy.parquet</td><td>part-00005-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-228-1-c000.snappy.parquet</td><td>398710</td><td>1684687266000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/vishal.abnave@borregaard.com/people.parquet/_SUCCESS",
         "_SUCCESS",
         0,
         1684687267000
        ],
        [
         "dbfs:/user/vishal.abnave@borregaard.com/people.parquet/_committed_3162966097481226355",
         "_committed_3162966097481226355",
         624,
         1684687267000
        ],
        [
         "dbfs:/user/vishal.abnave@borregaard.com/people.parquet/_started_3162966097481226355",
         "_started_3162966097481226355",
         0,
         1684687265000
        ],
        [
         "dbfs:/user/vishal.abnave@borregaard.com/people.parquet/part-00000-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-223-1-c000.snappy.parquet",
         "part-00000-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-223-1-c000.snappy.parquet",
         408085,
         1684687266000
        ],
        [
         "dbfs:/user/vishal.abnave@borregaard.com/people.parquet/part-00001-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-224-1-c000.snappy.parquet",
         "part-00001-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-224-1-c000.snappy.parquet",
         752260,
         1684687266000
        ],
        [
         "dbfs:/user/vishal.abnave@borregaard.com/people.parquet/part-00002-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-225-1-c000.snappy.parquet",
         "part-00002-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-225-1-c000.snappy.parquet",
         408748,
         1684687266000
        ],
        [
         "dbfs:/user/vishal.abnave@borregaard.com/people.parquet/part-00003-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-226-1-c000.snappy.parquet",
         "part-00003-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-226-1-c000.snappy.parquet",
         748279,
         1684687266000
        ],
        [
         "dbfs:/user/vishal.abnave@borregaard.com/people.parquet/part-00004-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-227-1-c000.snappy.parquet",
         "part-00004-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-227-1-c000.snappy.parquet",
         405791,
         1684687266000
        ],
        [
         "dbfs:/user/vishal.abnave@borregaard.com/people.parquet/part-00005-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-228-1-c000.snappy.parquet",
         "part-00005-tid-3162966097481226355-059fff2c-d705-4466-a30d-f178173aef85-228-1-c000.snappy.parquet",
         398710,
         1684687266000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "(source, sasEntity, sasToken) = getAzureDataSource()\n",
    "spark.conf.set(sasEntity, sasToken)\n",
    "\n",
    "source = '/mnt/training'\n",
    "sourceFile = source + \"/dataframes/people-with-dups.txt\"\n",
    "\n",
    "dbutils.fs.mkdirs(userhome)\n",
    "destFile = userhome + \"/people.parquet\"\n",
    "\n",
    "# In case it already exists\n",
    "dbutils.fs.rm(destFile, True)\n",
    "\n",
    "# dropDuplicates() will likely introduce a shuffle, so it helps to reduce the number of post-shuffle partitions.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n",
    "\n",
    "# Okay, now we can read this thing.\n",
    "df = (spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"sep\", \":\")\n",
    "    .csv(sourceFile)\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "dedupedDF = (df\n",
    "  .select(col(\"*\"),\n",
    "      lower(col(\"firstName\")).alias(\"lcFirstName\"),\n",
    "      lower(col(\"lastName\")).alias(\"lcLastName\"),\n",
    "      lower(col(\"middleName\")).alias(\"lcMiddleName\"),\n",
    "      translate(col(\"ssn\"), \"-\", \"\").alias(\"ssnNums\")\n",
    "      # regexp_replace(col(\"ssn\"), \"-\", \"\").alias(\"ssnNums\")\n",
    "      # regexp_replace(col(\"ssn\"), \"\"\"^(\\d{3})(\\d{2})(\\d{4})$\"\"\", \"$1-$2-$3\").alias(\"ssnNums\")\n",
    "   )\n",
    "  .dropDuplicates([\"lcFirstName\", \"lcMiddleName\", \"lcLastName\", \"ssnNums\", \"gender\", \"birthDate\", \"salary\"])\n",
    "  .drop(\"lcFirstName\", \"lcMiddleName\", \"lcLastName\", \"ssnNums\")\n",
    ")\n",
    "\n",
    "# Now we can save the results. We'll also re-read them and count them, just as a final check.\n",
    "# Just for fun, we'll use the Snappy compression codec. It's not as compact as Gzip, but it's much faster.\n",
    "(dedupedDF.write\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"compression\", \"snappy\")\n",
    "   .parquet(destFile)\n",
    ")\n",
    "dedupedDF = spark.read.parquet(destFile)\n",
    "print(\"Total Records: {0:,}\".format( dedupedDF.count() ))\n",
    "\n",
    "display( dbutils.fs.ls(destFile) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd6308f-de8e-4348-bb75-91ba02294fd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/logo_spark_tiny.png) Validate Your Answer\n",
    "\n",
    "At the bare minimum, we can verify that you wrote the parquet file out to **destFile** and that you have the right number of records.\n",
    "\n",
    "Running the following cell to confirm your result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dfa69d6-3ae6-48b1-b7f5-a549e67e984c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 Expected 100000 Records was correct, your answer: 100000\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><div style=\"font-weight:bold; font-size:larger; border-bottom: 1px solid #f0f0f0\">Your Answers</div><table style='margin:0'><tr style='font-size:larger; white-space:pre'>\n",
       "                  <td>01 Expected 100000 Records:&nbsp;&nbsp;</td>\n",
       "                  <td style=\"color:green; text-align:center; font-weight:bold\">passed</td>\n",
       "                  <td style=\"white-space:pre; font-family: monospace\">&nbsp;&nbsp;100000</td>\n",
       "                </tr></table></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finalDF = spark.read.parquet(destFile)\n",
    "finalCount = finalDF.count()\n",
    "\n",
    "clearYourResults()\n",
    "validateYourAnswer(\"01 Expected 100000 Records\", 972882115, finalCount)\n",
    "summarizeYourResults()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.Exercise-Deduplication-of-Data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
