{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6863fca1-cd39-4ab0-88e7-fde76009970d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Initialized classroom variables & functions..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Initialized classroom variables & functions...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "val tags = com.databricks.logging.AttributionContext.current.tags\n",
    "\n",
    "//*******************************************\n",
    "// GET VERSION OF APACHE SPARK\n",
    "//*******************************************\n",
    "\n",
    "// Get the version of spark\n",
    "val Array(sparkMajorVersion, sparkMinorVersion, _) = spark.version.split(\"\"\"\\.\"\"\")\n",
    "\n",
    "// Set the major and minor versions\n",
    "spark.conf.set(\"com.databricks.training.spark.major-version\", sparkMajorVersion)\n",
    "spark.conf.set(\"com.databricks.training.spark.minor-version\", sparkMinorVersion)\n",
    "\n",
    "//*******************************************\n",
    "// GET VERSION OF DATABRICKS RUNTIME\n",
    "//*******************************************\n",
    "\n",
    "// Get the version of the Databricks Runtime\n",
    "val version = {\n",
    "  val dbr = com.databricks.spark.util.SparkServerContext.serverVersion.replace(\"dbr-\", \"\")\n",
    "  val scalaMinMajVer = util.Properties.versionNumberString\n",
    "  val index = scalaMinMajVer.lastIndexOf(\".\")\n",
    "  val len = scalaMinMajVer.length\n",
    "  dbr + \".x-scala\" + scalaMinMajVer.dropRight(len - index)\n",
    "}\n",
    "\n",
    "val runtimeVersion = if (version != \"\") {\n",
    "  spark.conf.set(\"com.databricks.training.job\", \"false\")\n",
    "  version\n",
    "} else {\n",
    "  spark.conf.set(\"com.databricks.training.job\", \"true\")\n",
    "  dbutils.widgets.get(\"sparkVersion\")\n",
    "}\n",
    "\n",
    "val runtimeVersions = runtimeVersion.split(\"\"\"-\"\"\")\n",
    "// The GPU and ML runtimes push the number of elements out to 5\n",
    "// so we need to account for every scenario here. There should\n",
    "// never be a case in which there is less than two so we can fail\n",
    "// with an helpful error message for <2 or >5\n",
    "val (dbrVersion, scalaVersion) = {\n",
    "  runtimeVersions match {\n",
    "    case Array(d, _, _, _, s) => (d, s.replace(\"scala\", \"\"))\n",
    "    case Array(d, _, _, s)    => (d, s.replace(\"scala\", \"\"))\n",
    "    case Array(d, _, s)       => (d, s.replace(\"scala\", \"\"))\n",
    "    case Array(d, s)          => (d, s.replace(\"scala\", \"\"))\n",
    "    case _ =>\n",
    "      throw new IllegalArgumentException(s\"\"\"Dataset-Mounts: Cannot parse version(s) from \"${runtimeVersions.mkString(\", \")}\".\"\"\")\n",
    "  }\n",
    "}\n",
    "val Array(dbrMajorVersion, dbrMinorVersion, _*) = dbrVersion.split(\"\"\"\\.\"\"\")\n",
    "\n",
    "// Set the the major and minor versions\n",
    "spark.conf.set(\"com.databricks.training.dbr.version\", version)\n",
    "spark.conf.set(\"com.databricks.training.dbr.major-version\", dbrMajorVersion)\n",
    "spark.conf.set(\"com.databricks.training.dbr.minor-version\", dbrMinorVersion)\n",
    "\n",
    "//*******************************************\n",
    "// GET USERNAME AND USERHOME\n",
    "//*******************************************\n",
    "\n",
    "// Get the user's name\n",
    "val name = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_USER, java.util.UUID.randomUUID.toString.replace(\"-\", \"\"))\n",
    "val username = if (name != \"unknown\") name else dbutils.widgets.get(\"databricksUsername\")\n",
    "\n",
    "val userhome = s\"dbfs:/user/$username\"\n",
    "\n",
    "// Set the user's name and home directory\n",
    "spark.conf.set(\"com.databricks.training.username\", username)\n",
    "spark.conf.set(\"com.databricks.training.userhome\", userhome)\n",
    "\n",
    "//**********************************\n",
    "// GET TAG VALUE\n",
    "// Find a given tag's value or return a supplied default value if not found\n",
    "//**********************************\n",
    "\n",
    "def getTagValue(tagName: String, defaultValue: String = null): String = {\n",
    "  val tags = com.databricks.logging.AttributionContext.current.tags\n",
    "  val values = tags.collect({ case (t, v) if t.name == tagName => v }).toSeq\n",
    "  values.size match {\n",
    "    case 0 => defaultValue\n",
    "    case _ => values.head.toString\n",
    "  }\n",
    "}\n",
    "\n",
    "//**********************************\n",
    "// GET EXPERIMENT ID\n",
    "// JobId fallback in production mode\n",
    "//**********************************\n",
    "\n",
    "def getExperimentId(): Long = {\n",
    "  val notebookId = getTagValue(\"notebookId\", null)\n",
    "  val jobId = getTagValue(\"jobId\", null)\n",
    "  \n",
    "  (notebookId != null) match { \n",
    "      case true => notebookId.toLong\n",
    "      case false => (jobId != null) match { \n",
    "        case true => jobId.toLong\n",
    "        case false => 0\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "spark.conf.set(\"com.databricks.training.experimentId\", getExperimentId())\n",
    "\n",
    "//**********************************\n",
    "// VARIOUS UTILITY FUNCTIONS\n",
    "//**********************************\n",
    "\n",
    "def assertSparkVersion(expMajor:Int, expMinor:Int):String = {\n",
    "  val major = spark.conf.get(\"com.databricks.training.spark.major-version\")\n",
    "  val minor = spark.conf.get(\"com.databricks.training.spark.minor-version\")\n",
    "\n",
    "  if ((major.toInt < expMajor) || (major.toInt == expMajor && minor.toInt < expMinor)) {\n",
    "    throw new Exception(s\"This notebook must be ran on Spark version $expMajor.$expMinor or better, found Spark $major.$minor\")\n",
    "  }\n",
    "  return s\"$major.$minor\"\n",
    "}\n",
    "\n",
    "def assertDbrVersion(expMajor:Int, expMinor:Int):String = {\n",
    "  val major = spark.conf.get(\"com.databricks.training.dbr.major-version\")\n",
    "  val minor = spark.conf.get(\"com.databricks.training.dbr.minor-version\")\n",
    "\n",
    "  if ((major.toInt < expMajor) || (major.toInt == expMajor && minor.toInt < expMinor)) {\n",
    "    throw new Exception(s\"This notebook must be ran on Databricks Runtime (DBR) version $expMajor.$expMinor or better, found $major.$minor.\")\n",
    "  }\n",
    "  return s\"$major.$minor\"\n",
    "}\n",
    "\n",
    "def assertIsMlRuntime():Unit = {\n",
    "  if (version.contains(\"-ml-\") == false) {\n",
    "    throw new RuntimeException(s\"This notebook must be ran on a Databricks ML Runtime, found $version.\")\n",
    "  }\n",
    "}\n",
    "\n",
    "// **********************************\n",
    "//  GET AZURE DATASOURCE\n",
    "// **********************************\n",
    "\n",
    "def getAzureDataSource(): (String,String,String) = {\n",
    "  val datasource = spark.conf.get(\"com.databricks.training.azure.datasource\").split(\"\\t\")\n",
    "  val source = datasource(0)\n",
    "  val sasEntity = datasource(1)\n",
    "  val sasToken = datasource(2)\n",
    "  return (source, sasEntity, sasToken)\n",
    "}\n",
    "\n",
    "def initializeBrowserSideStats(): Unit = {\n",
    "  import java.net.URLEncoder.encode\n",
    "  import scala.collection.Map\n",
    "  import org.json4s.DefaultFormats\n",
    "  import org.json4s.jackson.JsonMethods._\n",
    "  import org.json4s.jackson.Serialization.write\n",
    "  import org.json4s.JsonDSL._\n",
    "\n",
    "  implicit val formats: DefaultFormats = DefaultFormats\n",
    "\n",
    "  val tags = com.databricks.logging.AttributionContext.current.tags\n",
    "\n",
    "  // Get the user's name and home directory\n",
    "  val username = spark.conf.get(\"com.databricks.training.username\", \"unknown-username\")\n",
    "  val userhome = spark.conf.get(\"com.databricks.training.userhome\", \"unknown-userhome\")\n",
    "  \n",
    "  val courseName = spark.conf.get(\"com.databricks.training.courseName\", \"unknown-course\")\n",
    "  val moduleName = spark.conf.get(\"com.databricks.training.moduleName\", \"unknown-module\")\n",
    "\n",
    "  // Get the the major and minor versions\n",
    "  val dbrVersion = spark.conf.get(\"com.databricks.training.dbr.version\", \"0.0\")\n",
    "  val dbrMajorVersion = spark.conf.get(\"com.databricks.training.dbr.major-version\", \"0\")\n",
    "  val dbrMinorVersion = spark.conf.get(\"com.databricks.training.dbr.minor-version\", \"0\")\n",
    "\n",
    "  val sessionId = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_SESSION_ID, \"unknown-sessionId\")\n",
    "  val hostName = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_HOST_NAME, \"unknown-host-name\")\n",
    "  val clusterMemory = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_CLUSTER_MEMORY, \"unknown-cluster-memory\")\n",
    "  val clientBranchName = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_BRANCH_NAME, \"unknown-branch-name\")\n",
    "  val notebookLanguage = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_NOTEBOOK_LANGUAGE, \"unknown-notebook-language\")\n",
    "  val browserUserAgent = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_USER_AGENT, \"unknown-user-agent\")\n",
    "  val browserHostName = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_HOST_NAME, \"unknown-host-name\")\n",
    "\n",
    "// Need to find docs or JAR file for com.databricks.logging.BaseTagDefinitions.TAG_ definitions  \n",
    "// Guessing TAG_BRANCH_NAME == clientBranchName\n",
    "//   val clientBranchName = (tags.filter(tup => tup._1.toString.contains(\"clientBranchName\")).map(tup => tup._2).head)\n",
    "// Guessing TAG_USER_AGENT == browserUserAgent\n",
    "//   val browserUserAgent = (tags.filter(tup => tup._1.toString.contains(\"browserUserAgent\")).map(tup => tup._2).head)\n",
    "// Guessing TAG_HOST_NAME == browserHostName\n",
    "//   val browserHostName = (tags.filter(tup => tup._1.toString.contains(\"browserHostName\")).map(tup => tup._2).head)\n",
    "\n",
    "// No TAG_ matches for these - wrap in try/catch if necessary\n",
    "  val sourceIpAddress = try { (tags.filter(tup => tup._1.toString.contains(\"sourceIpAddress\")).map(tup => tup._2).head) } catch { case e: Exception => \"unknown-source-ip\"}\n",
    "  val browserHash = try { (tags.filter(tup => tup._1.toString.contains(\"browserHash\")).map(tup => tup._2).head) } catch { case e: Exception => \"unknown-browser-hash\"}\n",
    "\n",
    "  val json = Map(\n",
    "    \"time\" -> java.time.Instant.now.toEpochMilli,\n",
    "    \"username\" -> username,\n",
    "    \"userhome\" -> userhome,\n",
    "    \"dbrVersion\" -> s\"$dbrMajorVersion.$dbrMinorVersion\",\n",
    "    \"tags\" -> tags.map(tup => (tup._1.name, tup._2))\n",
    "  )\n",
    "\n",
    "  val jsonString = write(json)\n",
    "  val tags_dump = write(tags.map(tup => (tup._1.name, tup._2)))\n",
    "  \n",
    "  val utf8 = java.nio.charset.StandardCharsets.UTF_8.toString;\n",
    "  \n",
    "  val html = s\"\"\"\n",
    "<html>\n",
    "<head>\n",
    "  <script src=\"https://files.training.databricks.com/static/js/classroom-support.min.js\"></script>\n",
    "  <script>\n",
    "<!--  \n",
    "    window.setTimeout( // Defer until bootstrap has enough time to async load\n",
    "      function(){ \n",
    "          Cookies.set(\"_academy_username\", \"$username\", {\"domain\":\".databricksusercontent.com\"});\n",
    "          Cookies.set(\"_academy_module_name\", \"$moduleName\", {\"domain\":\".databricksusercontent.com\"});\n",
    "          Cookies.set(\"_academy_course_name\", \"$courseName\", {\"domain\":\".databricksusercontent.com\"});\n",
    "          Cookies.set(\"_academy_sessionId\", \"$sessionId\", {\"domain\":\".databricksusercontent.com\"});\n",
    "          Cookies.set(\"_academy_hostName\", '$hostName', {\"domain\":\".databricksusercontent.com\"});\n",
    "          Cookies.set(\"_academy_clusterMemory\", '$clusterMemory', {\"domain\":\".databricksusercontent.com\"});\n",
    "          Cookies.set(\"_academy_clientBranchName\", '$clientBranchName', {\"domain\":\".databricksusercontent.com\"});\n",
    "          Cookies.set(\"_academy_notebookLanguage\", '$notebookLanguage', {\"domain\":\".databricksusercontent.com\"});\n",
    "          Cookies.set(\"_academy_sourceIpAddress\", '$sourceIpAddress', {\"domain\":\".databricksusercontent.com\"});\n",
    "          Cookies.set(\"_academy_browserUserAgent\", '$browserUserAgent', {\"domain\":\".databricksusercontent.com\"});\n",
    "          Cookies.set(\"_academy_browserHostName\", '$browserHostName', {\"domain\":\".databricksusercontent.com\"});\n",
    "          Cookies.set(\"_academy_browserHash\", '$browserHash', {\"domain\":\".databricksusercontent.com\"});\n",
    "          Cookies.set(\"_academy_tags\", $jsonString, {\"domain\":\".databricksusercontent.com\"});\n",
    "      }, 2000\n",
    "    );\n",
    "-->    \n",
    "  </script>\n",
    "</head>\n",
    "<body>\n",
    "  Class Setup Complete\n",
    "<script>\n",
    "</script>  \n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "displayHTML(html)\n",
    "  \n",
    "}\n",
    "\n",
    "def showStudentSurvey():Unit = {\n",
    "  import java.net.URLEncoder.encode\n",
    "  val utf8 = java.nio.charset.StandardCharsets.UTF_8.toString;\n",
    "  val username = encode(spark.conf.get(\"com.databricks.training.username\", \"unknown-user\"), utf8)\n",
    "  val courseName = encode(spark.conf.get(\"com.databricks.training.courseName\", \"unknown-course\"), utf8)\n",
    "  val moduleNameUnencoded = spark.conf.get(\"com.databricks.training.moduleName\", \"unknown-module\")\n",
    "  val moduleName = encode(moduleNameUnencoded, utf8)\n",
    "\n",
    "  import scala.collection.Map\n",
    "  import org.json4s.DefaultFormats\n",
    "  import org.json4s.jackson.JsonMethods._\n",
    "  import org.json4s.jackson.Serialization.write\n",
    "  import org.json4s.JsonDSL._\n",
    "\n",
    "  implicit val formats: DefaultFormats = DefaultFormats\n",
    "\n",
    "  val json = Map(\n",
    "    \"courseName\" -> courseName,\n",
    "    \"moduleName\" -> moduleName, // || \"unknown\",\n",
    "    \"name\" -> name,\n",
    "    \"time\" -> java.time.Instant.now.toEpochMilli,\n",
    "    \"username\" -> username,\n",
    "    \"userhome\" -> userhome,\n",
    "    \"dbrVersion\" -> s\"$dbrMajorVersion.$dbrMinorVersion\",\n",
    "    \"tags\" -> tags.map(tup => (tup._1.name, tup._2))\n",
    "  )\n",
    "\n",
    "  val jsonString = write(json)\n",
    "  val feedbackUrl = s\"https://engine-prod.databricks.training/feedback/$username/$courseName/$moduleName/\";\n",
    "  \n",
    "  val html = s\"\"\"\n",
    "  <html>\n",
    "  <head>\n",
    "    <script src=\"https://files.training.databricks.com/static/js/classroom-support.min.js\"></script>\n",
    "    <script>\n",
    "<!--    \n",
    "      window.setTimeout( // Defer until bootstrap has enough time to async load\n",
    "        () => { \n",
    "        //console.log($jsonString);\n",
    "        //console.log(JSON.stringify(\"$jsonString\");\n",
    "          var allCookies = Cookies.get();\n",
    "          Cookies.set(\"_academy_module_name\", \"$moduleName\", {\"domain\":\".databricksusercontent.com\"});\n",
    "\n",
    "          $$(\"#divComment\").css(\"display\", \"visible\");\n",
    "\n",
    "          // Emulate radio-button like feature for multiple_choice\n",
    "          $$(\".multiple_choicex\").on(\"click\", (evt) => {\n",
    "                const container = $$(evt.target).parent();\n",
    "                $$(\".multiple_choicex\").removeClass(\"checked\"); \n",
    "                $$(\".multiple_choicex\").removeClass(\"checkedRed\"); \n",
    "                $$(\".multiple_choicex\").removeClass(\"checkedGreen\"); \n",
    "                container.addClass(\"checked\"); \n",
    "                if (container.hasClass(\"thumbsDown\")) { \n",
    "                    container.addClass(\"checkedRed\"); \n",
    "                } else { \n",
    "                    container.addClass(\"checkedGreen\"); \n",
    "                };\n",
    "                \n",
    "                // Send the like/dislike before the comment is shown so we at least capture that.\n",
    "                // In analysis, always take the latest feedback for a module (if they give a comment, it will resend the like/dislike)\n",
    "                var json = { data: { liked: $$(\".multiple_choicex.checked\").attr(\"value\"), cookies: Cookies.get() } };\n",
    "                $$.ajax({\n",
    "                  type: 'PUT', \n",
    "                  url: '$feedbackUrl', \n",
    "                  data: JSON.stringify(json),\n",
    "                  dataType: 'json',\n",
    "                  processData: false\n",
    "                });\n",
    "                $$(\"#divComment\").show(\"fast\");\n",
    "          });\n",
    "\n",
    "\n",
    "           // Set click handler to do a PUT\n",
    "          $$(\"#btnSubmit\").on(\"click\", (evt) => {\n",
    "              // Use .attr(\"value\") instead of .val() - this is not a proper input box\n",
    "              var json = { data: { liked: $$(\".multiple_choicex.checked\").attr(\"value\"), comment: $$(\"#taComment\").val(), cookies: Cookies.get() } };\n",
    "\n",
    "              const msgThanks = \"Thank you for your feedback!\";\n",
    "              const msgError = \"There was an error submitting your feedback\";\n",
    "              const msgSending = \"Sending feedback...\";\n",
    "\n",
    "              $$(\"#feedback\").hide(\"fast\");\n",
    "              $$(\"#feedback-response\").html(msgSending);\n",
    "\n",
    "              $$.ajax({\n",
    "                type: 'PUT', \n",
    "                url: '$feedbackUrl', \n",
    "                data: JSON.stringify(json),\n",
    "                dataType: 'json',\n",
    "                processData: false\n",
    "              })\n",
    "                .done(function() {\n",
    "                  $$(\"#feedback-response\").html(msgThanks);\n",
    "                })\n",
    "                .fail(function() {\n",
    "                  $$(\"#feedback-response\").html(msgError);\n",
    "                })\n",
    "                ; // End of .ajax chain\n",
    "          });\n",
    "        }, 2000\n",
    "      );\n",
    "-->\n",
    "    </script>    \n",
    "    <style>\n",
    ".multiple_choicex > img:hover {    \n",
    "    background-color: white;\n",
    "    border-width: 0.15em;\n",
    "    border-radius: 5px;\n",
    "    border-style: solid;\n",
    "}\n",
    ".multiple_choicex.choice1 > img:hover {    \n",
    "    border-color: green;\n",
    "    background-color: green;\n",
    "}\n",
    ".multiple_choicex.choice2 > img:hover {    \n",
    "    border-color: red;\n",
    "    background-color: red;\n",
    "}\n",
    ".multiple_choicex {\n",
    "    margin: 1em;\n",
    "    padding: 0em;\n",
    "    background-color: white;\n",
    "    border: 0em;\n",
    "    border-style: solid;\n",
    "    border-color: green;\n",
    "}\n",
    ".multiple_choicex.checked {\n",
    "    border: 0.15em solid black;\n",
    "    background-color: white;\n",
    "    border-width: 0.5em;\n",
    "    border-radius: 5px;\n",
    "}\n",
    ".multiple_choicex.checkedGreen {\n",
    "    border-color: green;\n",
    "    background-color: green;\n",
    "}\n",
    ".multiple_choicex.checkedRed {\n",
    "    border-color: red;\n",
    "    background-color: red;\n",
    "}\n",
    "    </style>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h2 style=\"font-size:28px; line-height:34.3px\"><img style=\"vertical-align:middle\" src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"/>What did you think?</h2>\n",
    "    <p>Please let us know how if you liked this module, <b>$moduleNameUnencoded</b></p>\n",
    "    <div id=\"feedback\" style=\"clear:both;display:table;\">\n",
    "      <span class=\"multiple_choicex choice1 thumbsUp\" value=\"true\"><img style=\"width:100px\" src=\"https://files.training.databricks.com/images/feedback/thumbs-up.png\"/></span>\n",
    "      <span class=\"multiple_choicex choice2 thumbsDown\" value=\"false\"><img style=\"width:100px\" src=\"https://files.training.databricks.com/images/feedback/thumbs-down.png\"/></span>\n",
    "      <span>\n",
    "        <div id=\"divComment\" style=\"display:none\">\n",
    "          <textarea id=\"taComment\" placeholder=\"How can we make this module better? (optional)\" style=\"margin:1em;width:100%;height:200px;display:block\"></textarea>\n",
    "          <button id=\"btnSubmit\">Send</button>\n",
    "        </div>\n",
    "      </span>\n",
    "    </div>\n",
    "    <div id=\"feedback-response\" style=\"color:green; margin-top: 1em\">&nbsp;</div>\n",
    "  </body>\n",
    "  </html>\n",
    "  \"\"\"\n",
    "  displayHTML(html);  \n",
    "}\n",
    "\n",
    "class StudentsStatsService() extends org.apache.spark.scheduler.SparkListener {\n",
    "  import org.apache.spark.scheduler._\n",
    "\n",
    "  val hostname = \"engine-prod.databricks.training\"\n",
    "\n",
    "  def logEvent(eventType: String):Unit = {\n",
    "    import org.apache.http.entity._\n",
    "    import org.apache.http.impl.client.{HttpClients}\n",
    "    import org.apache.http.client.methods.HttpPost\n",
    "    import java.net.URLEncoder.encode\n",
    "    import org.json4s.jackson.Serialization\n",
    "    implicit val formats = org.json4s.DefaultFormats\n",
    "\n",
    "    var client:org.apache.http.impl.client.CloseableHttpClient = null\n",
    "\n",
    "    try {\n",
    "      val utf8 = java.nio.charset.StandardCharsets.UTF_8.toString;\n",
    "      val username = encode(spark.conf.get(\"com.databricks.training.username\", \"unknown-user\"), utf8)\n",
    "      val courseName = encode(spark.conf.get(\"com.databricks.training.courseName\", \"unknown-course\"), utf8)\n",
    "      val moduleName = encode(spark.conf.get(\"com.databricks.training.moduleName\", \"unknown-module\"), utf8)\n",
    "      val event = encode(eventType, utf8)\n",
    "      val url = s\"https://$hostname/tracking/$courseName/$moduleName/$username/$event\"\n",
    "    \n",
    "      val content = Map(\n",
    "        \"tags\" -> tags.map(tup => (tup._1.name, s\"$tup._2\")),\n",
    "        \"courseName\" -> courseName, \n",
    "        \"moduleName\" -> moduleName,\n",
    "        \"username\" -> username,\n",
    "        \"eventType\" -> eventType,\n",
    "        \"eventTime\" -> s\"${System.currentTimeMillis}\"\n",
    "      )\n",
    "      \n",
    "      val output = Serialization.write(content)\n",
    "    \n",
    "      // Future: (clues from Brian) \n",
    "      // Threadpool - don't use defaultExecutionContext; create our own EC; EC needs to be in scope as an implicit (Future calls will pick it up)\n",
    "      // apply() on Future companion\n",
    "      // onSuccess(), onFailure() (get exception from failure); map over future, final future, onComplete() gives Try object (can )\n",
    "      //    Future {\n",
    "      val client = HttpClients.createDefault()\n",
    "      val httpPost = new HttpPost(url)\n",
    "      val entity = new StringEntity(Serialization.write(Map(\"data\" -> content)))      \n",
    "\n",
    "      httpPost.setEntity(entity)\n",
    "      httpPost.setHeader(\"Accept\", \"application/json\")\n",
    "      httpPost.setHeader(\"Content-type\", \"application/json\")\n",
    "\n",
    "      client.execute(httpPost)\n",
    "      \n",
    "    } catch {\n",
    "      case e:Exception => org.apache.log4j.Logger.getLogger(getClass).error(\"Databricks Academey stats service failure\", e)\n",
    "      \n",
    "    } finally {\n",
    "      if (client != null) {\n",
    "        try { client.close() } \n",
    "        catch { case _:Exception => () }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = logEvent(\"JobEnd\" + jobEnd.jobId)\n",
    "  override def onJobStart(jobStart: SparkListenerJobStart): Unit = logEvent(\"JobStart: \" + jobStart.jobId)\n",
    "}\n",
    "\n",
    "val studentStatsService = new StudentsStatsService()\n",
    "if (spark.conf.get(\"com.databricks.training.studentStatsService.registered\", null) != \"registered\") {\n",
    "  sc.addSparkListener(studentStatsService)\n",
    "  spark.conf.set(\"com.databricks.training.studentStatsService\", \"registered\")\n",
    "}\n",
    "studentStatsService.logEvent(\"Classroom-Setup\")\n",
    "  \n",
    "//*******************************************\n",
    "// CHECK FOR REQUIRED VERIONS OF SPARK & DBR\n",
    "//*******************************************\n",
    "\n",
    "assertDbrVersion(4, 0)\n",
    "assertSparkVersion(2, 3)\n",
    "\n",
    "displayHTML(\"Initialized classroom variables & functions...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4893c494-17db-40b4-97dc-66f15de46044",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "#**********************************\n",
    "# VARIOUS UTILITY FUNCTIONS\n",
    "#**********************************\n",
    "\n",
    "def assertSparkVersion(expMajor, expMinor):\n",
    "  major = spark.conf.get(\"com.databricks.training.spark.major-version\")\n",
    "  minor = spark.conf.get(\"com.databricks.training.spark.minor-version\")\n",
    "\n",
    "  if (int(major) < expMajor) or (int(major) == expMajor and int(minor) < expMinor):\n",
    "    msg = \"This notebook must run on Spark version {}.{} or better, found.\".format(expMajor, expMinor, major, minor)\n",
    "    raise Exception(msg)\n",
    "    \n",
    "  return major+\".\"+minor\n",
    "\n",
    "def assertDbrVersion(expMajor, expMinor):\n",
    "  major = spark.conf.get(\"com.databricks.training.dbr.major-version\")\n",
    "  minor = spark.conf.get(\"com.databricks.training.dbr.minor-version\")\n",
    "\n",
    "  if (int(major) < expMajor) or (int(major) == expMajor and int(minor) < expMinor):\n",
    "    msg = \"This notebook must run on Databricks Runtime (DBR) version {}.{} or better, found.\".format(expMajor, expMinor, major, minor)\n",
    "    raise Exception(msg)\n",
    "    \n",
    "  return major+\".\"+minor\n",
    "\n",
    "def assertIsMlRuntime():\n",
    "  version = spark.conf.get(\"com.databricks.training.dbr.version\")\n",
    "  if \"-ml-\" not in version:\n",
    "    raise Exception(\"This notebook must be ran on a Databricks ML Runtime, found {}.\".format(version))\n",
    "\n",
    "    \n",
    "#**********************************\n",
    "# GET AZURE DATASOURCE\n",
    "#**********************************\n",
    "\n",
    "\n",
    "def getAzureDataSource(): \n",
    "  datasource = spark.conf.get(\"com.databricks.training.azure.datasource\").split(\"\\t\")\n",
    "  source = datasource[0]\n",
    "  sasEntity = datasource[1]\n",
    "  sasToken = datasource[2]\n",
    "  return (source, sasEntity, sasToken)\n",
    "\n",
    "    \n",
    "#**********************************\n",
    "# GET EXPERIMENT ID\n",
    "#**********************************\n",
    "\n",
    "def getExperimentId():\n",
    "  return spark.conf.get(\"com.databricks.training.experimentId\")\n",
    "\n",
    "#**********************************\n",
    "# INIT VARIOUS VARIABLES\n",
    "#**********************************\n",
    "\n",
    "username = spark.conf.get(\"com.databricks.training.username\", \"unknown-username\")\n",
    "userhome = spark.conf.get(\"com.databricks.training.userhome\", \"unknown-userhome\")\n",
    "\n",
    "import sys\n",
    "pythonVersion = spark.conf.set(\"com.databricks.training.python-version\", sys.version[0:sys.version.index(\" \")])\n",
    "\n",
    "None # suppress output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7cb061e-833b-47ee-bf84-b336cd96a5e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Mounted datasets to <b>/mnt/training</b> from <b>wasbs://training@dbtrainwesteurope.blob.core.windows.net/<b>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Mounted datasets to <b>/mnt/training</b> from <b>wasbs://training@dbtrainwesteurope.blob.core.windows.net/<b>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "\n",
    "//**********************************\n",
    "// CREATE THE MOUNTS\n",
    "//**********************************\n",
    "\n",
    "def getAwsRegion():String = {\n",
    "  try {\n",
    "    import scala.io.Source\n",
    "    import scala.util.parsing.json._\n",
    "\n",
    "    val jsonString = Source.fromURL(\"http://169.254.169.254/latest/dynamic/instance-identity/document\").mkString // reports ec2 info\n",
    "    val map = JSON.parseFull(jsonString).getOrElse(null).asInstanceOf[Map[Any,Any]]\n",
    "    map.getOrElse(\"region\", null).asInstanceOf[String]\n",
    "\n",
    "  } catch {\n",
    "    // We will use this later to know if we are Amazon vs Azure\n",
    "    case _: java.io.FileNotFoundException => null\n",
    "  }\n",
    "}\n",
    "\n",
    "def getAzureRegion():String = {\n",
    "  import com.databricks.backend.common.util.Project\n",
    "  import com.databricks.conf.trusted.ProjectConf\n",
    "  import com.databricks.backend.daemon.driver.DriverConf\n",
    "\n",
    "  new DriverConf(ProjectConf.loadLocalConfig(Project.Driver)).region\n",
    "}\n",
    "\n",
    "val awsAccessKey = \"AKIAJBRYNXGHORDHZB4A\"\n",
    "val awsSecretKey = \"a0BzE1bSegfydr3%2FGE3LSPM6uIV5A4hOUfpH8aFF\"\n",
    "val awsAuth = s\"${awsAccessKey}:${awsSecretKey}\"\n",
    "\n",
    "def getAwsMapping(region:String):(String,Map[String,String]) = {\n",
    "\n",
    "  val MAPPINGS = Map(\n",
    "    \"ap-northeast-1\" -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-ap-northeast-1/common\", Map[String,String]()),\n",
    "    \"ap-northeast-2\" -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-ap-northeast-2/common\", Map[String,String]()),\n",
    "    \"ap-south-1\"     -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-ap-south-1/common\", Map[String,String]()),\n",
    "    \"ap-southeast-1\" -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-ap-southeast-1/common\", Map[String,String]()),\n",
    "    \"ap-southeast-2\" -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-ap-southeast-2/common\", Map[String,String]()),\n",
    "    \"ca-central-1\"   -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-ca-central-1/common\", Map[String,String]()),\n",
    "    \"eu-central-1\"   -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-eu-central-1/common\", Map[String,String]()),\n",
    "    \"eu-west-1\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-eu-west-1/common\", Map[String,String]()),\n",
    "    \"eu-west-2\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-eu-west-2/common\", Map[String,String]()),\n",
    "\n",
    "    // eu-west-3 in Paris isn't supported by Databricks yet - not supported by the current version of the AWS library\n",
    "    // \"eu-west-3\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-eu-west-3/common\", Map[String,String]()),\n",
    "    \n",
    "    // Use Frankfurt in EU-Central-1 instead\n",
    "    \"eu-west-3\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-eu-central-1/common\", Map[String,String]()),\n",
    "    \n",
    "    \"sa-east-1\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-sa-east-1/common\", Map[String,String]()),\n",
    "    \"us-east-1\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-us-east-1/common\", Map[String,String]()),\n",
    "    \"us-east-2\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-us-east-2/common\", Map[String,String]()),\n",
    "    \"us-west-2\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training/common\", Map[String,String]()),\n",
    "    \"_default\"       -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training/common\", Map[String,String]())\n",
    "  )\n",
    "\n",
    "  MAPPINGS.getOrElse(region, MAPPINGS(\"_default\"))\n",
    "}\n",
    "\n",
    "def getAzureMapping(region:String):(String,Map[String,String]) = {\n",
    "\n",
    "  var MAPPINGS = Map(\n",
    "    \"australiacentral\"    -> (\"dbtrainaustraliasoutheas\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=br8%2B5q2ZI9osspeuPtd3haaXngnuWPnZaHKFoLmr370%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"australiacentral2\"   -> (\"dbtrainaustraliasoutheas\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=br8%2B5q2ZI9osspeuPtd3haaXngnuWPnZaHKFoLmr370%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"australiaeast\"       -> (\"dbtrainaustraliaeast\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=FM6dy59nmw3f4cfN%2BvB1cJXVIVz5069zHmrda5gZGtU%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"australiasoutheast\"  -> (\"dbtrainaustraliasoutheas\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=br8%2B5q2ZI9osspeuPtd3haaXngnuWPnZaHKFoLmr370%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"canadacentral\"       -> (\"dbtraincanadacentral\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=dwAT0CusWjvkzcKIukVnmFPTmi4JKlHuGh9GEx3OmXI%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"canadaeast\"          -> (\"dbtraincanadaeast\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=SYmfKBkbjX7uNDnbSNZzxeoj%2B47PPa8rnxIuPjxbmgk%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"centralindia\"        -> (\"dbtraincentralindia\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=afrYm3P5%2BB4gMg%2BKeNZf9uvUQ8Apc3T%2Bi91fo/WOZ7E%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"centralus\"           -> (\"dbtraincentralus\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=As9fvIlVMohuIV8BjlBVAKPv3C/xzMRYR1JAOB%2Bbq%2BQ%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"eastasia\"            -> (\"dbtraineastasia\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=sK7g5pki8bE88gEEsrh02VGnm9UDlm55zTfjZ5YXVMc%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"eastus\"              -> (\"dbtraineastus\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=tlw5PMp1DMeyyBGTgZwTbA0IJjEm83TcCAu08jCnZUo%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"eastus2\"             -> (\"dbtraineastus2\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=Y6nGRjkVj6DnX5xWfevI6%2BUtt9dH/tKPNYxk3CNCb5A%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"japaneast\"           -> (\"dbtrainjapaneast\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=q6r9MS/PC9KLZ3SMFVYO94%2BfM5lDbAyVsIsbBKEnW6Y%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"japanwest\"           -> (\"dbtrainjapanwest\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=M7ic7/jOsg/oiaXfo8301Q3pt9OyTMYLO8wZ4q8bko8%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"northcentralus\"      -> (\"dbtrainnorthcentralus\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=GTLU0g3pajgz4dpGUhOpJHBk3CcbCMkKT8wxlhLDFf8%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"northcentralus\"      -> (\"dbtrainnorthcentralus\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=GTLU0g3pajgz4dpGUhOpJHBk3CcbCMkKT8wxlhLDFf8%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"northeurope\"         -> (\"dbtrainnortheurope\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=35yfsQBGeddr%2BcruYlQfSasXdGqJT3KrjiirN/a3dM8%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"southcentralus\"      -> (\"dbtrainsouthcentralus\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=3cnVg/lzWMx5XGz%2BU4wwUqYHU5abJdmfMdWUh874Grc%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"southcentralus\"      -> (\"dbtrainsouthcentralus\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=3cnVg/lzWMx5XGz%2BU4wwUqYHU5abJdmfMdWUh874Grc%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"southindia\"          -> (\"dbtrainsouthindia\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=0X0Ha9nFBq8qkXEO0%2BXd%2B2IwPpCGZrS97U4NrYctEC4%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"southeastasia\"       -> (\"dbtrainsoutheastasia\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=H7Dxi1yqU776htlJHbXd9pdnI35NrFFsPVA50yRC9U0%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"uksouth\"             -> (\"dbtrainuksouth\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=SPAI6IZXmm%2By/WMSiiFVxp1nJWzKjbBxNc5JHUz1d1g%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"ukwest\"              -> (\"dbtrainukwest\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=olF4rjQ7V41NqWRoK36jZUqzDBz3EsyC6Zgw0QWo0A8%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"westcentralus\"       -> (\"dbtrainwestcentralus\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=UP0uTNZKMCG17IJgJURmL9Fttj2ujegj%2BrFN%2B0OszUE%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"westeurope\"          -> (\"dbtrainwesteurope\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=csG7jGsNFTwCArDlsaEcU4ZUJFNLgr//VZl%2BhdSgEuU%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"westindia\"           -> (\"dbtrainwestindia\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=fI6PNZ7YvDGKjArs1Et2rAM2zgg6r/bsKEjnzQxgGfA%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"westus\"              -> (\"dbtrainwestus\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=%2B1XZDXbZqnL8tOVsmRtWTH/vbDAKzih5ThvFSZMa3Tc%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"westus2\"             -> (\"dbtrainwestus2\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=DD%2BO%2BeIZ35MO8fnh/fk4aqwbne3MAJ9xh9aCIU/HiD4%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n",
    "    \"_default\"            -> (\"dbtrainwestus2\",\n",
    "                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=DD%2BO%2BeIZ35MO8fnh/fk4aqwbne3MAJ9xh9aCIU/HiD4%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\")\n",
    "  )\n",
    "\n",
    "  val (account: String, sasKey: String) = MAPPINGS.getOrElse(region, MAPPINGS(\"_default\"))\n",
    "\n",
    "  val blob = \"training\"\n",
    "  val source = s\"wasbs://$blob@$account.blob.core.windows.net/\"\n",
    "  val configMap = Map(\n",
    "    s\"fs.azure.sas.$blob.$account.blob.core.windows.net\" -> sasKey\n",
    "  )\n",
    "\n",
    "  (source, configMap)\n",
    "}\n",
    "\n",
    "def mountFailed(msg:String): Unit = {\n",
    "  println(msg)\n",
    "}\n",
    "\n",
    "def retryMount(source: String, mountPoint: String): Unit = {\n",
    "  try { \n",
    "    // Mount with IAM roles instead of keys for PVC\n",
    "    dbutils.fs.mount(source, mountPoint)\n",
    "  } catch {\n",
    "    case e: Exception => mountFailed(s\"*** ERROR: Unable to mount $mountPoint: ${e.getMessage}\")\n",
    "  }\n",
    "}\n",
    "\n",
    "def mount(source: String, extraConfigs:Map[String,String], mountPoint: String): Unit = {\n",
    "  try {\n",
    "    dbutils.fs.mount(source, mountPoint, extraConfigs=extraConfigs)\n",
    "  } catch {\n",
    "    case ioe: java.lang.IllegalArgumentException => retryMount(source, mountPoint)\n",
    "    case e: Exception => mountFailed(s\"*** ERROR: Unable to mount $mountPoint: ${e.getMessage}\")\n",
    "  }\n",
    "}\n",
    "\n",
    "def autoMount(fix:Boolean = false, failFast:Boolean = false, mountDir:String = \"/mnt/training\"): Unit = {\n",
    "  var awsRegion = getAwsRegion()\n",
    "\n",
    "  val (source, extraConfigs) = if (awsRegion != null)  {\n",
    "    spark.conf.set(\"com.databricks.training.region.name\", awsRegion)\n",
    "    getAwsMapping(awsRegion)\n",
    "\n",
    "  } else {\n",
    "    val azureRegion = getAzureRegion()\n",
    "    spark.conf.set(\"com.databricks.training.region.name\", azureRegion)\n",
    "    initAzureDataSource(azureRegion)\n",
    "  }\n",
    "  \n",
    "  val resultMsg = mountSource(fix, failFast, mountDir, source, extraConfigs)\n",
    "  displayHTML(resultMsg)\n",
    "}\n",
    "\n",
    "def initAzureDataSource(azureRegion:String):(String,Map[String,String]) = {\n",
    "  val mapping = getAzureMapping(azureRegion)\n",
    "  val (source, config) = mapping\n",
    "  val (sasEntity, sasToken) = config.head\n",
    "\n",
    "  val datasource = \"%s\\t%s\\t%s\".format(source, sasEntity, sasToken)\n",
    "  spark.conf.set(\"com.databricks.training.azure.datasource\", datasource)\n",
    "\n",
    "  return mapping\n",
    "}\n",
    "\n",
    "def mountSource(fix:Boolean, failFast:Boolean, mountDir:String, source:String, extraConfigs:Map[String,String]): String = {\n",
    "  val mntSource = source.replace(awsAuth+\"@\", \"\")\n",
    "\n",
    "  if (dbutils.fs.mounts().map(_.mountPoint).contains(mountDir)) {\n",
    "    val mount = dbutils.fs.mounts().filter(_.mountPoint == mountDir).head\n",
    "    if (mount.source == mntSource) {\n",
    "      return s\"\"\"Datasets are already mounted to <b>$mountDir</b> from <b>$mntSource</b>\"\"\"\n",
    "      \n",
    "    } else if (failFast) {\n",
    "      throw new IllegalStateException(s\"Expected $mntSource but found ${mount.source}\")\n",
    "      \n",
    "    } else if (fix) {\n",
    "      println(s\"Unmounting existing datasets ($mountDir from $mntSource)\")\n",
    "      dbutils.fs.unmount(mountDir)\n",
    "      mountSource(fix, failFast, mountDir, source, extraConfigs)\n",
    "\n",
    "    } else {\n",
    "      return s\"\"\"<b style=\"color:red\">Invalid Mounts!</b></br>\n",
    "                      <ul>\n",
    "                      <li>The training datasets you are using are from an unexpected source</li>\n",
    "                      <li>Expected <b>$mntSource</b> but found <b>${mount.source}</b></li>\n",
    "                      <li>Failure to address this issue may result in significant performance degradation. To address this issue:</li>\n",
    "                      <ol>\n",
    "                        <li>Insert a new cell after this one</li>\n",
    "                        <li>In that new cell, run the command <code style=\"color:blue; font-weight:bold\">%scala fixMounts()</code></li>\n",
    "                        <li>Verify that the problem has been resolved.</li>\n",
    "                      </ol>\"\"\"\n",
    "    }\n",
    "  } else {\n",
    "    println(s\"\"\"Mounting datasets to $mountDir from $mntSource\"\"\")\n",
    "    mount(source, extraConfigs, mountDir)\n",
    "    return s\"\"\"Mounted datasets to <b>$mountDir</b> from <b>$mntSource<b>\"\"\"\n",
    "  }\n",
    "}\n",
    "\n",
    "def fixMounts(): Unit = {\n",
    "  autoMount(true)\n",
    "}\n",
    "\n",
    "autoMount(true)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Dataset-Mounts",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
