{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d5104c8-9eff-4e4b-b73a-d50039cd431f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Introduction to DataFrames Lab\n",
    "## Distinct Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace863ec-b26a-4114-8bf0-08c77612ebaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Instructions\n",
    "\n",
    "In the cell provided below, write the code necessary to count the number of distinct articles in our data set.\n",
    "0. Copy and paste all you like from the previous notebook.\n",
    "0. Read in our parquet files.\n",
    "0. Apply the necessary transformations.\n",
    "0. Assign the count to the variable `totalArticles`\n",
    "0. Run the last cell to verify that the data was loaded correctly.\n",
    "\n",
    "**Bonus**\n",
    "\n",
    "If you recall from the beginning of the previous notebook, the act of reading in our parquet files will trigger a job.\n",
    "0. Define a schema that matches the data we are working with.\n",
    "0. Update the read operation to use the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df07601-fe93-404c-9e0d-3a7dd5011381",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n",
    "\n",
    "Run the following cell to configure our \"classroom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3950e52d-6fe6-4f78-bf78-cb8cf6b5ca8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Initialized classroom variables & functions..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Initialized classroom variables & functions...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8fafe29-0cc8-44b4-a611-4753e4a01ec4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/training/ has been unmounted.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Mounted datasets to <b>/mnt/training</b> from <b>wasbs://training@dbtrainwesteurope.blob.core.windows.net/<b>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Mounted datasets to <b>/mnt/training</b> from <b>wasbs://training@dbtrainwesteurope.blob.core.windows.net/<b>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mount \"/mnt/training\" again using \"%run \"./Includes/Dataset-Mounts-New\"\" if it is failed in \"./Includes/Classroom-Setup\"\n",
    "try:\n",
    "    files = dbutils.fs.ls(\"/mnt/training\")\n",
    "except:\n",
    "    dbutils.fs.unmount('/mnt/training/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79953634-ba19-433c-8815-74ca7be347fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Created user-specific database"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Created user-specific database",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Using the database <b style=\"color:green\">vishal_abnave_borregaard_com_db</b>."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Using the database <b style=\"color:green\">vishal_abnave_borregaard_com_db</b>.",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "All done!"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "All done!",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Dataset-Mounts-New\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d275369-2f4d-4066-95c2-db921fb06d09",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Show Your Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5556d94-8c2b-4346-8738-1b510c3cfd32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(source, sasEntity, sasToken) = getAzureDataSource()\n",
    "spark.conf.set(sasEntity, sasToken)\n",
    "\n",
    "source = '/mnt/training'\n",
    "path = source + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d23378b-e939-4447-b857-c4ade01217e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Articles: 1,783,138\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Replace <<FILL_IN>> with your code. \n",
    "\n",
    "df = (spark                    # Our SparkSession & Entry Point\n",
    "  .read                        # Our DataFrameReader\n",
    "  .parquet(path)                  # Read in the parquet files\n",
    "  .select(\"article\")                  # Reduce the columns to just the one\n",
    "  .distinct()                  # Produce a unique set of values\n",
    ")\n",
    "totalArticles = df.count() # Identify the total number of records remaining.\n",
    "\n",
    "print(\"Distinct Articles: {0:,}\".format(totalArticles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa831b52-1d9e-4d90-b6f8-0bea718d45b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Verify Your Work\n",
    "Run the following cell to verify that your `DataFrame` was created properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca72e54-f41f-453a-9826-09641824eb34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expected = 1783138\n",
    "assert totalArticles == expected, \"Expected the total to be \" + str(expected) + \" but found \" + str(totalArticles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "434aecff-3836-444b-8138-105fbc6842aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Bonus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7026aa70-4d4d-4a3e-8d04-2b5a6b1c918b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Articles: 1,783,138\n"
     ]
    }
   ],
   "source": [
    "# Bonus> Read with user defined schema for reducing the jobs\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "parqSchema = StructType(\n",
    "  [\n",
    "    StructField('project', StringType(), True),\n",
    "    StructField('article', StringType(), True),\n",
    "    StructField('requests', IntegerType(), True),\n",
    "    StructField('bytes_served', LongType(), True)\n",
    "  ]\n",
    ")\n",
    "\n",
    "df = (spark                    # Our SparkSession & Entry Point\n",
    "  .read                        # Our DataFrameReader\n",
    "  .schema(parqSchema)\n",
    "  .parquet(path)                  # Read in the parquet files\n",
    "  .select(\"article\")                  # Reduce the columns to just the one\n",
    "  .distinct()                  # Produce a unique set of values\n",
    ")\n",
    "totalArticles = df.count() # Identify the total number of records remaining.\n",
    "\n",
    "print(\"Distinct Articles: {0:,}\".format(totalArticles))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.Exercise: Distinct Articles",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
