{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f90e17-15f5-4e93-8c7f-1317d6d1fe3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Reading Data Lab\n",
    "* The goal of this lab is to put into practice some of what you have learned about reading data with Apache Spark.\n",
    "* The instructions are provided below along with empty cells for you to do your work.\n",
    "* At the bottom of this notebook are additional cells that will help verify that your work is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8eeaf3d-560d-49c2-966c-86f9522884d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Instructions\n",
    "0. Start with the file **dbfs:/mnt/training/wikipedia/clickstream/2015_02_clickstream.tsv**, some random file you haven't seen yet.\n",
    "0. Read in the data and assign it to a `DataFrame` named **testDF**.\n",
    "0. Run the last cell to verify that the data was loaded correctly and to print its schema.\n",
    "0. The one untestable requirement is that you should be able to create the `DataFrame` and print its schema **without** executing a single job.\n",
    "\n",
    "**Note:** For the test to pass, the following columns should have the specified data types:\n",
    " * **prev_id**: integer\n",
    " * **curr_id**: integer\n",
    " * **n**: integer\n",
    " * **prev_title**: string\n",
    " * **curr_title**: string\n",
    " * **type**: string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ff5a08-fa65-484b-a629-4cf2511d5cdf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n",
    "\n",
    "Run the following cell to configure our \"classroom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "065ed616-90c2-4a45-8706-f520e117869e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Initialized classroom variables & functions..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Initialized classroom variables & functions...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"../Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddee253d-d0a0-4f35-850d-f8261eea1a59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/training/ has been unmounted.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Mounted datasets to <b>/mnt/training</b> from <b>wasbs://training@dbtrainwesteurope.blob.core.windows.net/<b>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Mounted datasets to <b>/mnt/training</b> from <b>wasbs://training@dbtrainwesteurope.blob.core.windows.net/<b>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mount \"/mnt/training\" again using \"%run \"./Includes/Dataset-Mounts-New\"\" if it is failed in \"./Includes/Classroom-Setup\"\n",
    "try:\n",
    "    files = dbutils.fs.ls(\"/mnt/training\")\n",
    "except:\n",
    "    dbutils.fs.unmount('/mnt/training/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09a65ed8-6ec2-480a-a366-baa06898a3bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Created user-specific database"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Created user-specific database",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Using the database <b style=\"color:green\">vishal_abnave_borregaard_com_db</b>."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Using the database <b style=\"color:green\">vishal_abnave_borregaard_com_db</b>.",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "All done!"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "All done!",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"../Includes/Dataset-Mounts-New\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efde0dd2-9660-4b08-911c-2e028b7ba4a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Show Your Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0931c77a-af01-45c3-ad79-1c30c804ae1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- prev_id: integer (nullable = true)\n |-- curr_id: integer (nullable = true)\n |-- n: integer (nullable = true)\n |-- prev_title: string (nullable = true)\n |-- curr_title: string (nullable = true)\n |-- type: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "\n",
    "# The students will actually need to do this in two steps.\n",
    "fileName = \"dbfs:/mnt/training/wikipedia/clickstream/2015_02_clickstream.tsv\"\n",
    "\n",
    "# The first step will be to use inferSchema = true \n",
    "# It's the only way to figure out what the column and data types are\n",
    "(spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .csv(fileName)\n",
    "  .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "024e30fc-8cf9-46c0-9a6e-784a503c9ed1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- prev_id: integer (nullable = true)\n |-- curr_id: integer (nullable = true)\n |-- n: integer (nullable = true)\n |-- prev_title: string (nullable = true)\n |-- curr_title: string (nullable = true)\n |-- type: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# The second step is to create the schema\n",
    "schema = StructType([\n",
    "    StructField(\"prev_id\", IntegerType(), False),\n",
    "    StructField(\"curr_id\", IntegerType(), False),\n",
    "    StructField(\"n\", IntegerType(), False),\n",
    "    StructField(\"prev_title\", StringType(), False),\n",
    "    StructField(\"curr_title\", StringType(), False),\n",
    "    StructField(\"type\", StringType(), False)\n",
    "])\n",
    "\n",
    "fileName = \"dbfs:/mnt/training/wikipedia/clickstream/2015_02_clickstream.tsv\"\n",
    "\n",
    "#The third step is to read the data in with the user-defined schema\n",
    "testDF = (spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(schema)\n",
    "  .csv(fileName)\n",
    ")\n",
    "\n",
    "testDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cc2030f-a5b4-45b7-826b-039ccdc65816",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Verify Your Work\n",
    "Run the following cell to verify that your `DataFrame` was created properly.\n",
    "\n",
    "**Remember:** This should execute without triggering a single job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16856786-7256-4175-94f1-fa03c0b4b09e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- prev_id: integer (nullable = true)\n |-- curr_id: integer (nullable = true)\n |-- n: integer (nullable = true)\n |-- prev_title: string (nullable = true)\n |-- curr_title: string (nullable = true)\n |-- type: string (nullable = true)\n\nCongratulations, all tests passed... that is if no jobs were triggered :-)\n\n"
     ]
    }
   ],
   "source": [
    "testDF.printSchema()\n",
    "\n",
    "columns = testDF.dtypes\n",
    "assert len(columns) == 6, \"Expected 6 columns but found \" + str(len(columns))\n",
    "\n",
    "assert columns[0][0] == \"prev_id\",    \"Expected column 0 to be \\\"prev_id\\\" but found \\\"\" + columns[0][0] + \"\\\".\"\n",
    "assert columns[0][1] == \"int\",        \"Expected column 0 to be of type \\\"int\\\" but found \\\"\" + columns[0][1] + \"\\\".\"\n",
    "\n",
    "assert columns[1][0] == \"curr_id\",    \"Expected column 1 to be \\\"curr_id\\\" but found \\\"\" + columns[1][0] + \"\\\".\"\n",
    "assert columns[1][1] == \"int\",        \"Expected column 1 to be of type \\\"int\\\" but found \\\"\" + columns[1][1] + \"\\\".\"\n",
    "\n",
    "assert columns[2][0] == \"n\",          \"Expected column 2 to be \\\"n\\\" but found \\\"\" + columns[2][0] + \"\\\".\"\n",
    "assert columns[2][1] == \"int\",        \"Expected column 2 to be of type \\\"int\\\" but found \\\"\" + columns[2][1] + \"\\\".\"\n",
    "\n",
    "assert columns[3][0] == \"prev_title\", \"Expected column 3 to be \\\"prev_title\\\" but found \\\"\" + columns[3][0] + \"\\\".\"\n",
    "assert columns[3][1] == \"string\",     \"Expected column 3 to be of type \\\"string\\\" but found \\\"\" + columns[3][1] + \"\\\".\"\n",
    "\n",
    "assert columns[4][0] == \"curr_title\", \"Expected column 4 to be \\\"curr_title\\\" but found \\\"\" + columns[4][0] + \"\\\".\"\n",
    "assert columns[4][1] == \"string\",     \"Expected column 4 to be of type \\\"string\\\" but found \\\"\" + columns[4][1] + \"\\\".\"\n",
    "\n",
    "assert columns[5][0] == \"type\",       \"Expected column 5 to be \\\"type\\\" but found \\\"\" + columns[5][0] + \"\\\".\"\n",
    "assert columns[5][1] == \"string\",     \"Expected column 5 to be of type \\\"string\\\" but found \\\"\" + columns[5][1] + \"\\\".\"\n",
    "\n",
    "print(\"Congratulations, all tests passed... that is if no jobs were triggered :-)\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Solution - Reading Data 8 - Lab",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
