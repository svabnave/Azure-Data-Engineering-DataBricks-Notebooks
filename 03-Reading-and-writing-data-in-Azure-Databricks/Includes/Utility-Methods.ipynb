{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc9ce8b6-d7b8-490c-a679-669033641c44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# ****************************************************************************\n",
    "# Utility method to count & print the number of records in each partition.\n",
    "# ****************************************************************************\n",
    "\n",
    "def printRecordsPerPartition(df):\n",
    "  def countInPartition(iterator): yield __builtin__.sum(1 for _ in iterator)\n",
    "  results = (df.rdd                   # Convert to an RDD\n",
    "    .mapPartitions(countInPartition)  # For each partition, count\n",
    "    .collect()                        # Return the counts to the driver\n",
    "  )\n",
    "  \n",
    "  print(\"Per-Partition Counts\")\n",
    "  i = 0\n",
    "  for result in results: \n",
    "    i = i + 1\n",
    "    print(\"#{}: {:,}\".format(i, result))\n",
    "  \n",
    "# ****************************************************************************\n",
    "# Utility to count the number of files in and size of a directory\n",
    "# ****************************************************************************\n",
    "\n",
    "def computeFileStats(path):\n",
    "  bytes = 0\n",
    "  count = 0\n",
    "\n",
    "  files = dbutils.fs.ls(path)\n",
    "  \n",
    "  while (len(files) > 0):\n",
    "    fileInfo = files.pop(0)\n",
    "    if (fileInfo.isDir() == False):               # isDir() is a method on the fileInfo object\n",
    "      count += 1\n",
    "      bytes += fileInfo.size                      # size is a parameter on the fileInfo object\n",
    "    else:\n",
    "      files.extend(dbutils.fs.ls(fileInfo.path))  # append multiple object to files\n",
    "      \n",
    "  return (count, bytes)\n",
    "\n",
    "# ****************************************************************************\n",
    "# Utility method to cache a table with a specific name\n",
    "# ****************************************************************************\n",
    "\n",
    "def cacheAs(df, name, level):\n",
    "  from pyspark.sql.utils import AnalysisException\n",
    "  print(\"WARNING: The PySpark API currently does not allow specification of the storage level - using MEMORY-ONLY\")\n",
    "  \n",
    "  try: spark.catalog.uncacheTable(name)\n",
    "  except AnalysisException: None\n",
    "  \n",
    "  df.createOrReplaceTempView(name)\n",
    "  spark.catalog.cacheTable(name)\n",
    "  #spark.catalog.cacheTable(name, level)\n",
    "  return df\n",
    "\n",
    "\n",
    "# ****************************************************************************\n",
    "# Simplified benchmark of count()\n",
    "# ****************************************************************************\n",
    "\n",
    "def benchmarkCount(func):\n",
    "  import time\n",
    "  start = float(time.time() * 1000)                    # Start the clock\n",
    "  df = func()\n",
    "  total = df.count()                                   # Count the records\n",
    "  duration = float(time.time() * 1000) - start         # Stop the clock\n",
    "  return (df, total, duration)\n",
    "\n",
    "\n",
    "# ****************************************************************************\n",
    "# Utility method to wait until the stream is read\n",
    "# ****************************************************************************\n",
    "\n",
    "def untilStreamIsReady(name):\n",
    "  queries = list(filter(lambda query: query.name == name, spark.streams.active))\n",
    "\n",
    "  if len(queries) == 0:\n",
    "    print(\"The stream is not active.\")\n",
    "\n",
    "  else:\n",
    "    while (queries[0].isActive and len(queries[0].recentProgress) == 0):\n",
    "      pass # wait until there is any type of progress\n",
    "\n",
    "    if queries[0].isActive:\n",
    "      print(\"The stream is active and ready.\")\n",
    "    else:\n",
    "      print(\"The stream is not active.\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f23cfff-89cb-489f-b1a3-f9650c28527e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<div>Declared various utility methods:</div>\n",
       "<li>Declared <b style=\"color:green\">printRecordsPerPartition(<i>df:DataFrame</i>)</b> for diagnostics</li>\n",
       "<li>Declared <b style=\"color:green\">computeFileStats(<i>path:String</i>)</b> returns <b style=\"color:green\">(count:Long, bytes:Long)</b> for diagnostics</li>\n",
       "<li>Declared <b style=\"color:green\">tracker</b> for benchmarking</li>\n",
       "<li>Declared <b style=\"color:green\">cacheAs(<i>df:DataFrame, name:String, level:StorageLevel</i>)</b> for better debugging</li>\n",
       "<li>Declared <b style=\"color:green\">benchmarkCount(<i>lambda:DataFrame</i>)</b> returns <b style=\"color:green\">(df:DataFrame, total:Long, duration:Long)</b> for diagnostics</li>\n",
       "<li>Declared <b style=\"color:green\">untilStreamIsReady(<i>name:String</i>)</b> to control workflow</li>\n",
       "<br/>\n",
       "<div>All done!</div>\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n<div>Declared various utility methods:</div>\n<li>Declared <b style=\"color:green\">printRecordsPerPartition(<i>df:DataFrame</i>)</b> for diagnostics</li>\n<li>Declared <b style=\"color:green\">computeFileStats(<i>path:String</i>)</b> returns <b style=\"color:green\">(count:Long, bytes:Long)</b> for diagnostics</li>\n<li>Declared <b style=\"color:green\">tracker</b> for benchmarking</li>\n<li>Declared <b style=\"color:green\">cacheAs(<i>df:DataFrame, name:String, level:StorageLevel</i>)</b> for better debugging</li>\n<li>Declared <b style=\"color:green\">benchmarkCount(<i>lambda:DataFrame</i>)</b> returns <b style=\"color:green\">(df:DataFrame, total:Long, duration:Long)</b> for diagnostics</li>\n<li>Declared <b style=\"color:green\">untilStreamIsReady(<i>name:String</i>)</b> to control workflow</li>\n<br/>\n<div>All done!</div>\n",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "\n",
    "// ****************************************************************************\n",
    "// Utility method to count & print the number of records in each partition.\n",
    "// ****************************************************************************\n",
    "\n",
    "def printRecordsPerPartition(df:org.apache.spark.sql.Dataset[Row]):Unit = {\n",
    "  // import org.apache.spark.sql.functions._\n",
    "  val results = df.rdd                                   // Convert to an RDD\n",
    "    .mapPartitions(it => Array(it.size).iterator, true)  // For each partition, count\n",
    "    .collect()                                           // Return the counts to the driver\n",
    "\n",
    "  println(\"Per-Partition Counts\")\n",
    "  var i = 0\n",
    "  for (r <- results) {\n",
    "    i = i +1\n",
    "    println(\"#%s: %,d\".format(i,r))\n",
    "  }\n",
    "}\n",
    "\n",
    "// ****************************************************************************\n",
    "// Utility to count the number of files in and size of a directory\n",
    "// ****************************************************************************\n",
    "\n",
    "def computeFileStats(path:String):(Long,Long) = {\n",
    "  var bytes = 0L\n",
    "  var count = 0L\n",
    "\n",
    "  import scala.collection.mutable.ArrayBuffer\n",
    "  var files=ArrayBuffer(dbutils.fs.ls(path):_ *)\n",
    "\n",
    "  while (files.isEmpty == false) {\n",
    "    val fileInfo = files.remove(0)\n",
    "    if (fileInfo.isDir == false) {\n",
    "      count += 1\n",
    "      bytes += fileInfo.size\n",
    "    } else {\n",
    "      files.append(dbutils.fs.ls(fileInfo.path):_ *)\n",
    "    }\n",
    "  }\n",
    "  (count, bytes)\n",
    "}\n",
    "\n",
    "// ****************************************************************************\n",
    "// Utility method to cache a table with a specific name\n",
    "// ****************************************************************************\n",
    "\n",
    "def cacheAs(df:org.apache.spark.sql.DataFrame, name:String, level:org.apache.spark.storage.StorageLevel):org.apache.spark.sql.DataFrame = {\n",
    "  try spark.catalog.uncacheTable(name)\n",
    "  catch { case _: org.apache.spark.sql.AnalysisException => () }\n",
    "  \n",
    "  df.createOrReplaceTempView(name)\n",
    "  spark.catalog.cacheTable(name, level)\n",
    "  return df\n",
    "}\n",
    "\n",
    "// ****************************************************************************\n",
    "// Simplified benchmark of count()\n",
    "// ****************************************************************************\n",
    "\n",
    "def benchmarkCount(func:() => org.apache.spark.sql.DataFrame):(org.apache.spark.sql.DataFrame, Long, Long) = {\n",
    "  val start = System.currentTimeMillis            // Start the clock\n",
    "  val df = func()                                 // Get our lambda\n",
    "  val total = df.count()                          // Count the records\n",
    "  val duration = System.currentTimeMillis - start // Stop the clock\n",
    "  (df, total, duration)\n",
    "}\n",
    "\n",
    "// ****************************************************************************\n",
    "// Benchmarking and cache tracking tool\n",
    "// ****************************************************************************\n",
    "\n",
    "case class JobResults[T](runtime:Long, duration:Long, cacheSize:Long, maxCacheBefore:Long, remCacheBefore:Long, maxCacheAfter:Long, remCacheAfter:Long, result:T) {\n",
    "  def printTime():Unit = {\n",
    "    if (runtime < 1000)                 println(f\"Runtime:  ${runtime}%,d ms\")\n",
    "    else if (runtime < 60 * 1000)       println(f\"Runtime:  ${runtime/1000.0}%,.2f sec\")\n",
    "    else if (runtime < 60 * 60 * 1000)  println(f\"Runtime:  ${runtime/1000.0/60.0}%,.2f min\")\n",
    "    else                                println(f\"Runtime:  ${runtime/1000.0/60.0/60.0}%,.2f hr\")\n",
    "    \n",
    "    if (duration < 1000)                println(f\"All Jobs: ${duration}%,d ms\")\n",
    "    else if (duration < 60 * 1000)      println(f\"All Jobs: ${duration/1000.0}%,.2f sec\")\n",
    "    else if (duration < 60 * 60 * 1000) println(f\"All Jobs: ${duration/1000.0/60.0}%,.2f min\")\n",
    "    else                                println(f\"Job Dur: ${duration/1000.0/60.0/60.0}%,.2f hr\")\n",
    "  }\n",
    "  def printCache():Unit = {\n",
    "    if (Math.abs(cacheSize) < 1024)                    println(f\"Cached:   ${cacheSize}%,d bytes\")\n",
    "    else if (Math.abs(cacheSize) < 1024 * 1024)        println(f\"Cached:   ${cacheSize/1024.0}%,.3f KB\")\n",
    "    else if (Math.abs(cacheSize) < 1024 * 1024 * 1024) println(f\"Cached:   ${cacheSize/1024.0/1024.0}%,.3f MB\")\n",
    "    else                                               println(f\"Cached:   ${cacheSize/1024.0/1024.0/1024.0}%,.3f GB\")\n",
    "    \n",
    "    println(f\"Before:   ${remCacheBefore / 1024.0 / 1024.0}%,.3f / ${maxCacheBefore / 1024.0 / 1024.0}%,.3f MB / ${100.0*remCacheBefore/maxCacheBefore}%.2f%%\")\n",
    "    println(f\"After:    ${remCacheAfter / 1024.0 / 1024.0}%,.3f / ${maxCacheAfter / 1024.0 / 1024.0}%,.3f MB / ${100.0*remCacheAfter/maxCacheAfter}%.2f%%\")\n",
    "  }\n",
    "  def print():Unit = {\n",
    "    printTime()\n",
    "    printCache()\n",
    "  }\n",
    "}\n",
    "\n",
    "case class Node(driver:Boolean, executor:Boolean, address:String, maximum:Long, available:Long) {\n",
    "  def this(address:String, maximum:Long, available:Long) = this(address.contains(\"-\"), !address.contains(\"-\"), address, maximum, available)\n",
    "}\n",
    "\n",
    "class Tracker() extends org.apache.spark.scheduler.SparkListener() {\n",
    "  \n",
    "  sc.addSparkListener(this)\n",
    "  \n",
    "  val jobStarts = scala.collection.mutable.Map[Int,Long]()\n",
    "  val jobEnds = scala.collection.mutable.Map[Int,Long]()\n",
    "  \n",
    "  def track[T](func:() => T):JobResults[T] = {\n",
    "    jobEnds.clear()\n",
    "    jobStarts.clear()\n",
    "\n",
    "    val executorsBefore = sc.getExecutorMemoryStatus.map(x => new Node(x._1, x._2._1, x._2._2)).filter(_.executor)\n",
    "    val maxCacheBefore = executorsBefore.map(_.maximum).sum\n",
    "    val remCacheBefore = executorsBefore.map(_.available).sum\n",
    "    \n",
    "    val start = System.currentTimeMillis()\n",
    "    val result = func()\n",
    "    val runtime = System.currentTimeMillis() - start\n",
    "    \n",
    "    Thread.sleep(1000) // give it a second to catch up\n",
    "\n",
    "    val executorsAfter = sc.getExecutorMemoryStatus.map(x => new Node(x._1, x._2._1, x._2._2)).filter(_.executor)\n",
    "    val maxCacheAfter = executorsAfter.map(_.maximum).sum\n",
    "    val remCacheAfter = executorsAfter.map(_.available).sum\n",
    "\n",
    "    var duration = 0L\n",
    "    \n",
    "    for ((jobId, startAt) <- jobStarts) {\n",
    "      assert(jobEnds.keySet.exists(_ == jobId), s\"A conclusion for Job ID $jobId was not found.\") \n",
    "      duration += jobEnds(jobId) - startAt\n",
    "    }\n",
    "    JobResults(runtime, duration, remCacheBefore-remCacheAfter, maxCacheBefore, remCacheBefore, maxCacheAfter, remCacheAfter, result)\n",
    "  }\n",
    "  override def onJobStart(jobStart: org.apache.spark.scheduler.SparkListenerJobStart):Unit = jobStarts.put(jobStart.jobId, jobStart.time)\n",
    "  override def onJobEnd(jobEnd: org.apache.spark.scheduler.SparkListenerJobEnd): Unit = jobEnds.put(jobEnd.jobId, jobEnd.time)\n",
    "}\n",
    "\n",
    "val tracker = new Tracker()\n",
    "\n",
    "\n",
    "// ****************************************************************************\n",
    "// Utility method to wait until the stream is read\n",
    "// ****************************************************************************\n",
    "\n",
    "def untilStreamIsReady(name:String):Unit = {\n",
    "  val queries = spark.streams.active.filter(_.name == name)\n",
    "\n",
    "  if (queries.length == 0) {\n",
    "    println(\"The stream is not active.\")\n",
    "  } else {\n",
    "    while (queries(0).isActive && queries(0).recentProgress.length == 0) {\n",
    "      // wait until there is any type of progress\n",
    "    }\n",
    "\n",
    "    if (queries(0).isActive) {\n",
    "      println(\"The stream is active and ready.\")\n",
    "    } else {\n",
    "      println(\"The stream is not active.\")\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "displayHTML(\"\"\"\n",
    "<div>Declared various utility methods:</div>\n",
    "<li>Declared <b style=\"color:green\">printRecordsPerPartition(<i>df:DataFrame</i>)</b> for diagnostics</li>\n",
    "<li>Declared <b style=\"color:green\">computeFileStats(<i>path:String</i>)</b> returns <b style=\"color:green\">(count:Long, bytes:Long)</b> for diagnostics</li>\n",
    "<li>Declared <b style=\"color:green\">tracker</b> for benchmarking</li>\n",
    "<li>Declared <b style=\"color:green\">cacheAs(<i>df:DataFrame, name:String, level:StorageLevel</i>)</b> for better debugging</li>\n",
    "<li>Declared <b style=\"color:green\">benchmarkCount(<i>lambda:DataFrame</i>)</b> returns <b style=\"color:green\">(df:DataFrame, total:Long, duration:Long)</b> for diagnostics</li>\n",
    "<li>Declared <b style=\"color:green\">untilStreamIsReady(<i>name:String</i>)</b> to control workflow</li>\n",
    "<br/>\n",
    "<div>All done!</div>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Utility-Methods",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
